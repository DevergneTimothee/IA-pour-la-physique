{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we are first going to generate data that are linearly separable, then we will perform a logistic regression on them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(12)\n",
    "num_observations = 5000\n",
    "\n",
    "x1 = np.random.multivariate_normal([0, 0], [[1, .75],[.75, 1]], num_observations)\n",
    "x2 = np.random.multivariate_normal([1, 4], [[1, .75],[.75, 1]], num_observations)\n",
    "\n",
    "simulated_separableish_features = np.vstack((x1, x2)).astype(np.float32)\n",
    "simulated_labels = np.hstack((np.zeros(num_observations),\n",
    "                              np.ones(num_observations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it looks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1],\n",
    "            c = simulated_labels, alpha = .4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Picking a Link Function\n",
    "Generalized linear models usually tranform a linear model of the predictors by using a [link function](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function). In logistic regression, the link function is the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function). We can implement this really easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(scores):\n",
    "    return 1 / (1 + np.exp(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maximizing the Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, we want to model a logistic like model from a binary classification problem as seen in lectures:\n",
    "$$P_{\\rm model}(l({\\bf x})=0) = \\frac 1{1+\\exp({\\bf \\theta} \\cdot {\\bf x})}~~~ \\text{&}~~~ P_{\\rm model}(l({\\bf x})=1) = \\frac {\\exp({\\bf \\theta} \\cdot {\\bf x})}{1+\\exp({\\bf \\theta} \\cdot {\\bf x})}$$\n",
    "\n",
    "We now want to use the likelihood function as a loss : \n",
    "$${\\rm Loss} = - \\sum_{i=1}^n  \\log(P_{\\rm y_i}(x^i)) $$\n",
    "where $y$ is the target class, $x_{i}$ represents an individual data point, and $\\theta$ is the weights vector.\n",
    "\n",
    "\n",
    "***Exercise*** Show that the loss can be written :\n",
    "$${\\rm Loss} =  \\sum_{\\rm dataset} - y_i {\\bf \\theta} \\cdot {\\bf x}_i  + \\log{(1+\\exp({\\bf \\theta} \\cdot {\\bf x}_i ))} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(features, target, weights):\n",
    "    scores = np.dot(features, weights)\n",
    "    ll = ###write here what you want\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Gradient\n",
    "\n",
    "Now we need an equation for the gradient of the log-likelihood. By taking the derivative of the equation above and reformulating in matrix form, the gradient becomes: \n",
    "\n",
    "***Exercise*** Compute the gradient of the loss function and show that it can be written in the following way :\n",
    "$$\\begin{equation}\n",
    "\\bigtriangledown {\\rm Loss } = -X^{T}(Y - Predictions)\n",
    "\\end{equation}$$\n",
    "Where $Predictions = 1 - \\sigma(\\theta.X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Logistic Regression Function\n",
    "Write the solver for the logistic regression using the gradient descent algorithm, for num_step steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):\n",
    "    if add_intercept:\n",
    "        intercept = np.ones((features.shape[0], 1))\n",
    "        features = np.hstack((intercept, features))\n",
    "        \n",
    "    weights = np.zeros(features.shape[1])\n",
    "        \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to do the regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intercept = np.ones((simulated_separableish_features.shape[0], 1))\n",
    "data_with_intercept = np.hstack((intercept, simulated_separableish_features))\n",
    "weights = logistic_regression(data_with_intercept, simulated_labels,\n",
    "                     num_steps = 50000, learning_rate = 5e-5, add_intercept=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Check the convergence of your algorithm\n",
    "\n",
    "***Exercise*** Separate the dataset into a training and test set and plot the evolution of the value of the loss over the number of iterations. It is widely used technique in machine learning so you need to start getting used to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modify the function above so that you can get the loss on both sets as a function of the number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise*** Make this plot for different values of the learning rate and number of steps and comment your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise*** As usual in python someone has already done a way better job than us, look at the doc on how to do a logistic regression using some python library, and compare the weights from that function and your weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write here what you want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's the Accuracy?\n",
    "To get the accuracy, I just need to use the final weights to get the logits for the dataset (`final_scores`). Then I can use `sigmoid` to get the final predictions and round them to the nearest integer (0 or 1) to get the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def line(x,a,b,c):\n",
    "    return -x*b/c-a/c\n",
    "def myline(x):\n",
    "    return line(x,weights[0],weights[1],weights[2])\n",
    "\n",
    "final_scores = np.dot(data_with_intercept, weights)\n",
    "preds = np.round(sigmoid(final_scores))\n",
    "\n",
    "print('Accuracy: {0}'.format((preds == simulated_labels).sum().astype(float) / len(preds)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly perfect (which makes sense given the data). We should only have made mistakes right in the middle between the clusters. Let's make sure that's what happened. In the following plot, blue points are correct predictions, and red points are incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (12, 8))\n",
    "plt.scatter(simulated_separableish_features[:, 0], simulated_separableish_features[:, 1],\n",
    "            c = preds == simulated_labels - 1, alpha = .8, s = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
