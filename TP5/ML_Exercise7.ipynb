{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0inPLHOFCDn6"
   },
   "source": [
    "# Machine Learning Exercise 7 - K-Means Clustering & PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOeWkYmZCDoA"
   },
   "source": [
    "In this exercise we'll implement K-means clustering and use it to compress an image.  We'll start with a simple 2D data set to see how K-means works, then we'll apply it to image compression.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y349oZWICDoA"
   },
   "source": [
    "## K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0y61pwNCDoB"
   },
   "source": [
    "To start out we're going to implement and apply K-means to a simple 2-dimensional data set to gain some intuition about how it works.  K-means is an iterative, unsupervised clustering algorithm that groups similar instances together into clusters.  The algorithm starts by guessing the initial centroids for each cluster, and then repeatedly assigns instances to the nearest cluster and re-computes the centroid of that cluster.  The first piece that we're going to implement is a function that finds the closest centroid for each instance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vW5ZNOPzCDoC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "from scipy.io import loadmat\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtR7-e2rCDoE"
   },
   "outputs": [],
   "source": [
    "def find_closest_centroids(X, centroids):\n",
    "    m = X.shape[0]\n",
    "    k = centroids.shape[0]\n",
    "    idx = np.zeros(m)\n",
    "    \n",
    "    #CODE A FUNCTION THAT RETURNS FOR EVERY POINT THE CLOSEST CENTROID\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zchLskYhCDoF"
   },
   "source": [
    "Let's test the function to make sure it's working as expected.  We'll use the test case provided in the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EAcfBswHCDoG",
    "outputId": "28bcbe95-bfe6-489f-ea60-fa26d83fb617"
   },
   "outputs": [],
   "source": [
    "data = loadmat('data/ex7data2.mat')\n",
    "X = data['X']\n",
    "initial_centroids = initial_centroids = np.array([[3, 3], [6, 2], [8, 5]])\n",
    "\n",
    "idx = find_closest_centroids(X, initial_centroids)\n",
    "idx[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OXePGpOdCDoI"
   },
   "source": [
    "The output matches the expected values in the text (remember our arrays are zero-indexed instead of one-indexed so the values are one lower than in the exercise).  Next we need a function to compute the centroid of a cluster.  The centroid is simply the mean of all of the examples currently assigned to the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wktWaNYVCDoJ"
   },
   "outputs": [],
   "source": [
    "def compute_centroids(X, idx, k):\n",
    "    m, n = X.shape\n",
    "    centroids = np.zeros((k, n))\n",
    "    #CODE A FUNCTION THAT COMPUTES THE NEW CENTROIDS\n",
    "    \n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfehzQ2mCDoJ",
    "outputId": "8774fea1-90df-4bf4-c21b-ac46e4a1207d"
   },
   "outputs": [],
   "source": [
    "compute_centroids(X, idx, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KW7C6bdyCDoK"
   },
   "source": [
    "This output also matches the expected values from the exercise.  So far so good.  The next part involves actually running the algorithm for some number of iterations and visualizing the result.  This step was implmented for us in the exercise, but since it's not that complicated I'll build it here from scratch.  In order to run the algorithm we just need to alternate between assigning examples to the nearest cluster and re-computing the cluster centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HK7j6ZVlCDoK"
   },
   "outputs": [],
   "source": [
    "def run_k_means(X, initial_centroids, max_iters):\n",
    "    m, n = X.shape\n",
    "    k = initial_centroids.shape[0]\n",
    "    idx = np.zeros(m)\n",
    "    centroids = initial_centroids\n",
    "    #CODE A FUNCTION THAT IMPLEMENTS THE K-MEANS ALGORITHM\n",
    "    return idx, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aUDlPjVaCDoL"
   },
   "outputs": [],
   "source": [
    "idx, centroids = run_k_means(X, initial_centroids, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUe_jvzkCDoL",
    "outputId": "32eacb6a-7549-4a86-be95-4e65c8975d58"
   },
   "outputs": [],
   "source": [
    "cluster1 = X[np.where(idx == 0)[0],:]\n",
    "cluster2 = X[np.where(idx == 1)[0],:]\n",
    "cluster3 = X[np.where(idx == 2)[0],:]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(cluster1[:,0], cluster1[:,1], s=30, color='r', label='Cluster 1')\n",
    "ax.scatter(cluster2[:,0], cluster2[:,1], s=30, color='g', label='Cluster 2')\n",
    "ax.scatter(cluster3[:,0], cluster3[:,1], s=30, color='b', label='Cluster 3')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LV2YR1S-CDoM"
   },
   "source": [
    "One step we skipped over is a process for initializing the centroids.  This can affect the convergence of the algorithm.  We're tasked with creating a function that selects random examples and uses them as the initial centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zs_hrSs6CDoM"
   },
   "outputs": [],
   "source": [
    "def init_centroids(X, k):\n",
    "    m, n = X.shape\n",
    "    centroids = np.zeros((k, n))\n",
    "    #CODE A FUNCTION THAT INITIALISES THE CENTROIDS RANDOMLY\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHnS9IRvCDoN",
    "outputId": "795a1339-4b8e-481f-9a53-2b8155e08176"
   },
   "outputs": [],
   "source": [
    "init_centroids(X, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ukmi2axaCDoN"
   },
   "source": [
    "Our next task is to apply K-means to image compression.  The intuition here is that we can use clustering to find a small number of colors that are most representative of the image, and map the original 24-bit colors to a lower-dimensional color space using the cluster assignments.  Here's the image we're going to compress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P4-xGW6ACDoO",
    "outputId": "239e6278-0394-4ae2-c139-cd7dc21f017e"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='data/bird_small.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCVREiUKCDoO"
   },
   "source": [
    "The raw pixel data has been pre-loaded for us so let's pull it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXr5UpMkCDoP",
    "outputId": "7b23e47c-cfed-41aa-c081-65240711d91d"
   },
   "outputs": [],
   "source": [
    "image_data = loadmat('data/bird_small.mat')\n",
    "image_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wCWugraGCDoP",
    "outputId": "e51873c5-162c-4f75-def3-a45f736e4a03"
   },
   "outputs": [],
   "source": [
    "A = image_data['A']\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOdytyABCDoR"
   },
   "source": [
    "Now we need to apply some pre-processing to the data and feed it into the K-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "niBVKOdGCDoR",
    "outputId": "8e4d3b6b-a0fb-4dff-dff2-8019bbab034e"
   },
   "outputs": [],
   "source": [
    "# normalize value ranges\n",
    "A = A / 255.\n",
    "\n",
    "# reshape the array\n",
    "X = np.reshape(A, (A.shape[0] * A.shape[1], A.shape[2]))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UFWy-rXCDoR",
    "outputId": "0c8e6224-312e-4438-dedd-815322684eff"
   },
   "outputs": [],
   "source": [
    "# randomly initialize the centroids\n",
    "initial_centroids = init_centroids(X, 16)\n",
    "\n",
    "# run the algorithm\n",
    "idx, centroids = run_k_means(X, initial_centroids, 10)\n",
    "\n",
    "# get the closest centroids one last time\n",
    "idx = find_closest_centroids(X, centroids)\n",
    "\n",
    "# map each pixel to the centroid value\n",
    "X_recovered = centroids[idx.astype(int),:]\n",
    "X_recovered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y--LIugUCDoR",
    "outputId": "d94b59da-0fc2-44cc-bbc1-4c9b19bea399"
   },
   "outputs": [],
   "source": [
    "# reshape to the original dimensions\n",
    "X_recovered = np.reshape(X_recovered, (A.shape[0], A.shape[1], A.shape[2]))\n",
    "X_recovered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jd9ZehtlCDoR",
    "outputId": "08276d0e-6c9f-4f1d-e9f2-cdafc7cc71e4"
   },
   "outputs": [],
   "source": [
    "plt.imshow(X_recovered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ung3jECXCDoS"
   },
   "source": [
    "Cool!  You can see that we created some artifacts in the compression but the main features of the image are still there.  That's it for K-means.  We'll now move on to principal component analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4B-aHIjCDoS"
   },
   "source": [
    "## Principal component analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_ef0CgiCDoS"
   },
   "source": [
    "PCA is a linear transformation that finds the \"principal components\", or directions of greatest variance, in a data set.  It can be used for dimension reduction among other things.  In this exercise we're first tasked with implementing PCA and applying it to a simple 2-dimensional data set to see how it works.  Let's start off by loading and visualizing the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXp1SgUdCDoT",
    "outputId": "d579a0c3-02b7-452f-8d62-6c9573efd59c"
   },
   "outputs": [],
   "source": [
    "data = loadmat('data/ex7data1.mat')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1xGXd1gGCDoT",
    "outputId": "7067da1d-0886-4f8e-f626-31f3aab6a56e"
   },
   "outputs": [],
   "source": [
    "X = data['X']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(X[:, 0], X[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kA4oiRJzCDoU"
   },
   "source": [
    "The algorithm for PCA is fairly simple.  After ensuring that the data is normalized, the output is simply the singular value decomposition of the covariance matrix of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zuLGr5BNCDoU"
   },
   "outputs": [],
   "source": [
    "def pca(X):\n",
    "    # normalize the features\n",
    "    X = (X - X.mean()) / X.std()\n",
    "    \n",
    "    # compute the covariance matrix\n",
    "    X = np.matrix(X)\n",
    "    cov = (X.T * X) / X.shape[0]\n",
    "    \n",
    "    # perform SVD\n",
    "    U, S, V = np.linalg.svd(cov)\n",
    "    \n",
    "    return U, S, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rxkxLAObCDoU",
    "outputId": "97c75864-256f-47e1-ac88-022441771dc8"
   },
   "outputs": [],
   "source": [
    "U, S, V = pca(X)\n",
    "U, S, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ORFV79MCDoV"
   },
   "source": [
    "Now that we have the principal components (matrix U), we can use these to project the original data into a lower-dimensional space.  For this task we'll implement a function that computes the projection and selects only the top K components, effectively reducing the number of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JGSb5eKVCDoV"
   },
   "outputs": [],
   "source": [
    "def project_data(X, U, k):\n",
    "    U_reduced = U[:,:k]\n",
    "    return np.dot(X, U_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inu1mqaUCDoV",
    "outputId": "ca18b4ac-414a-4367-8af3-b658d1cfd595"
   },
   "outputs": [],
   "source": [
    "Z = project_data(X, U, 1)\n",
    "Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wbjb9Uk9CDoW"
   },
   "source": [
    "We can also attempt to recover the original data by reversing the steps we took to project it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FD90n48PCDoW"
   },
   "outputs": [],
   "source": [
    "def recover_data(Z, U, k):\n",
    "    U_reduced = U[:,:k]\n",
    "    return np.dot(Z, U_reduced.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1rLMXx8CDoW",
    "outputId": "5eb1ce4a-dbc4-4584-c077-f99f828c5f5d"
   },
   "outputs": [],
   "source": [
    "X_recovered = recover_data(Z, U, 1)\n",
    "X_recovered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XImvnCOUCDoX",
    "outputId": "c5a742d5-6b97-47dc-f38e-1b2e2ae1d931"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax.scatter(X_recovered[:, 0], X_recovered[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZ39hGP5CDoX"
   },
   "source": [
    "Notice that the projection axis for the first principal component was basically a diagonal line through the data set.  When we reduced the data to one dimension, we lost the variations around that diagonal line, so in our reproduction everything falls along that diagonal.\n",
    "\n",
    "Our last task in this exercise is to apply PCA to images of faces.  By using the same dimension reduction techniques we can capture the \"essence\" of the images using much less data than the original images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cw7dZ8MHCDoX",
    "outputId": "29db6ef8-26bd-4171-af0f-88ec9894cdc9"
   },
   "outputs": [],
   "source": [
    "faces = loadmat('data/ex7faces.mat')\n",
    "X = faces['X']\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVljpq8hCDoY"
   },
   "source": [
    "The exercise code includes a function that will render the first 100 faces in the data set in a grid.  Rather than try to re-produce that here, you can look in the exercise text for an example of what they look like.  We can at least render one image fairly easily though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8oIrekuQCDoY"
   },
   "outputs": [],
   "source": [
    "face = np.reshape(X[3,:], (32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b8eYoHgKCDoY",
    "outputId": "c19292af-476e-4a0c-e439-a9af092ffc61"
   },
   "outputs": [],
   "source": [
    "plt.imshow(face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uwJZpP6NCDoZ"
   },
   "source": [
    "Yikes, that looks awful.  These are only 32 x 32 grayscale images though (it's also rendering sideways, but we can ignore that for now).  Anyway's let's proceed.  Our next step is to run PCA on the faces data set and take the top 100 principal components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7uoXJmeCDoZ"
   },
   "outputs": [],
   "source": [
    "U, S, V = pca(X)\n",
    "Z = project_data(X, U, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cj9Ibfc4CDoZ"
   },
   "source": [
    "Now we can attempt to recover the original structure and render it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUFqbA5wCDoZ",
    "outputId": "a63dc475-b393-43a9-cc91-a9ea938ea03b"
   },
   "outputs": [],
   "source": [
    "X_recovered = recover_data(Z, U, 100)\n",
    "face = np.reshape(X_recovered[3,:], (32, 32))\n",
    "plt.imshow(face)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XlNwY06CDoZ"
   },
   "source": [
    "Observe that we lost some detail, though not as much as you might expect for a 10x reduction in the number of dimensions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "digits.data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can try to reduce the dimensionality of the dataset by using PCA and visualise it into a 2D plane. To do so, look into the scikit-learn for a function that performs PCA and apply it to perform PCA on the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADD YOUR CODE HERE\n",
    "#The results of the PCA will be stored in the array called projected\n",
    "\n",
    "\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=digits.target, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('Spectral', 10))\n",
    "plt.xlabel('component 1')\n",
    "plt.ylabel('component 2')\n",
    "plt.colorbar();\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose the number of components\n",
    "To find the right number of components to keep in your PCA, you can plot the cumulative sum of explained variance to see how much information is kept in your reduced data. Plot the cumulative explained variance as a function of the number of dimension kept and assess the number of dimensions you can keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA as a tool for compression\n",
    "Earlier, we used K-means to compress a picture, here, we can use PCA to compress data. Indeed, with the plot you drew in the previous picture, we see that by only keeping a few components, we still have most of the information. Write a piece of code to visualise how the resulting picture evolves with the number of components kept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(8, 8, figsize=(8, 8))\n",
    "fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ###fill this code using only one picture ####\n",
    "\n",
    "    ax.imshow(im.reshape((8, 8)), cmap='binary')\n",
    "    ax.text(0.95, 0.05, 'n = {0}'.format(i + 1), ha='right',\n",
    "            transform=ax.transAxes, color='green')\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA as a noise filter\n",
    "PCA can also be used as a tool to remove noise from data. Indeed, components that carry the most important part of the variance won't be impacted by the variance due to the noise. Therefore, if we keep only these dimensions, we should be able to rule out the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_digits(data):\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(10, 4),\n",
    "                             subplot_kw={'xticks':[], 'yticks':[]},\n",
    "                             gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(data[i].reshape(8, 8),\n",
    "                  cmap='binary', interpolation='nearest',\n",
    "                  clim=(0, 16))\n",
    "plot_digits(digits.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EXERCISE :** Now add some random normal noise to this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, perform PCA to keep only 50% of the variance on the noisy date. Once you have done this, perform the inverse transform of the PCA data to obtain the filtered version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw conclusions :"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "ML-Exercise7.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
